from flask import Flask, request, render_template
import PyPDF2
import re
import pickle

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from torch import nn
from torch.optim import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.utils import to_categorical
from flask_ngrok import run_with_ngrok

# Load the modified dataset
data = pd.read_csv('malurl_final.csv')

# Drop rows with missing values (NaN) in the 'class' column
data = data.dropna(subset=['class'])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data['url'], data['class'], test_size=0.2, random_state=42)

# Encode the target labels
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Tokenize the URLs
max_words = 1000
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)
X_train_sequences = tokenizer.texts_to_sequences(X_train)
X_test_sequences = tokenizer.texts_to_sequences(X_test)

# Pad the sequences
X_train_padded = pad_sequence([torch.LongTensor(seq) for seq in X_train_sequences], batch_first=True, padding_value=0)
X_test_padded = pad_sequence([torch.LongTensor(seq) for seq in X_test_sequences], batch_first=True, padding_value=0)

# Create a custom dataset
class URLDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Create a custom collate function to handle variable-length sequences
def collate_fn(batch):
    X = [item[0] for item in batch]
    y = [item[1] for item in batch]
    X_padded = pad_sequence([torch.LongTensor(seq) for seq in X], batch_first=True, padding_value=0)
    y_one_hot = torch.FloatTensor(np.eye(3)[y])
    return X_padded, y_one_hot

# Create a simple LSTM model
class URLClassifier(nn.Module):
    def __init__(self, max_words, embedding_dim, hidden_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(max_words, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        x = self.embedding(x)
        _, (h_n, _) = self.lstm(x)
        out = self.fc(h_n.squeeze(0))
        return out

# Initialize the model
model = URLClassifier(max_words, 128, 128, 3)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters())

# Create data loaders
train_dataset = URLDataset(X_train_sequences, y_train_encoded)
test_dataset = URLDataset(X_test_sequences, y_test_encoded)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)

# Load the trained model
model = pickle.load(open('model.pkl', 'rb'))

# Load the tokenizer
tokenizer = pickle.load(open('tokenizer.pkl', 'rb'))

# Define the app
app = Flask(__name__)
run_with_ngrok(app)

# Define the function to extract text from a PDF file
def extract_text_from_pdf(file):
    pdf_reader = PyPDF2.PdfReader(file)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text()
    return text

# Define the function to extract hyperlinks from text
def extract_links_from_text(text):
    # Regular expression to match hyperlinks and URLs
    regex = r"(?P<url>https?://[^\s]+)"

    # Find all matches of the regex in the text
    matches = re.finditer(regex, text, re.MULTILINE)

    # Store the matches in a dictionary
    links = {}
    link_num = 1
    for match in matches:
        url = match.group("url")
        links[link_num] = url
        link_num += 1
    return links

# Define the home page
@app.route('/')
def home():
    return render_template('index.html')

# Define the prediction page
@app.route('/predict', methods=['POST'])
def predict():
    # Get the uploaded file
    file = request.files['file']

    # Extract the text from the file
    text = extract_text_from_pdf(file)
    links = extract_links_from_text(text)

    # Preprocess the input
    link_classes = {}
    num_benign = 0
    num_defacement = 0
    num_phishing = 0

    for link_num, link in links.items():
        input_sequence = tokenizer.texts_to_sequences([link])
        input_padded = pad_sequences(input_sequence, maxlen=max_words, padding='post')

        # Pass the input through the model
        with torch.no_grad():
            model.eval()
            output = model(torch.LongTensor(input_padded))
            probabilities = nn.functional.softmax(output, dim=1).numpy()[0]

        # Get the predicted class label
        predicted_class = np.argmax(probabilities)
        link_classes[link] = predicted_class

        # Update the counters
        if predicted_class == 0:
            num_benign += 1
        elif predicted_class == 1:
            num_defacement += 1
        else:
            num_phishing += 1

    # Render the results page
    return render_template('results.html', link_classes=link_classes, num_benign=num_benign, num_defacement=num_defacement, num_phishing=num_phishing)

if __name__ == '__main__':
    app.run()
